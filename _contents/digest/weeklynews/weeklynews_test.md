# 📰 한 주 요약
매주 커뮤니티 리뷰를 거친, 이번 주 가장 가치있는 소식 모음입니다.
매주 수요일 업데이트 됩니다.
## 📆 [2025/07/07 ~ 2025/07/13] 주간 소식
## ⭐ 5점

### 1. Claude Code가 이제 훅(hooks)을 지원함
• Claude Code에 사용자 정의 훅 기능이 도입. LLM의 선택에 의존하지 않고, 앱의 행동을 더욱 정확하고 반복적으로 제어할 수 있음
• 알림 커스터마이징, 코드 자동 포맷팅, 명령 로그 추적과 같은 다양한 자동화가 가능
• 명령어 실행 전/후, 알림 발생, 응답 완료 시점 등에서 동작하며, 설정 파일을 통해 프로젝트·사용자·엔터프라이즈 레벨로 관리할 수 있음
• 설정 파일 구조와 매처(matcher) 방식을 통해, 특정 도구 호출 시점에 특정 훅만 실행할 수 있음
• 입력은 JSON 포맷으로 전달되고, 출력은 exit code 또는 JSON으로 결과·피드백을 제어함
• 훅은 셸 명령어를 사용자의 전체 권한으로 자동 실행하므로, 보안 및 안전에 대한 주의 필요함

>👉 **Why it matters:** 
> 내용 없음

### 2. Firestarter - 웹사이트 콘텐츠를 바탕으로 AI Chatbot을 만들어주는 오픈소스 프로젝트
Firestarter는 URL만 입력하면 웹사이트를 자동으로 크롤링·벡터화해 곧바로 RAG 기반 AI 챗봇을 배포해 주는 MIT 오픈소스 프로젝트다. Firecrawl로 콘텐츠를 Markdown 형태로 긁어오고, Upstash Search에 임베딩·인덱싱한 뒤 Next.js 15·Vercel AI SDK를 이용해 스트리밍 인터페이스와 OpenAI 호환 REST 엔드포인트를 제공한다.
사용자는 복잡한 인프라 없이도 /create API로 챗봇을 만들고 /query로 질의를 보낼 수 있으며, 생성된 챗봇은 OpenAI·Anthropic·Groq 순 우선순위에 따라 LLM을 호출한다. 크롤링 깊이·LLM 우선순위·챗봇 생성 활성화 여부 같은 파라미터는 firestarter.config.ts에서 간단히 조정할 수 있어 자가 호스팅이나 기능 확장이 용이하다.
DocsGPT / LangChain 같은 기존 RAG 툴과 달리 Firestarter는 **“즉시 사용 가능”**과 OpenAI-호환 API를 강점으로 내세운다. 개발자 문서, FAQ, 고객 지원 사이트를 빠르게 “대화형 데이터 소스”로 전환할 수 있어 커뮤니티·기업 채택이 빠르게 늘고 있다.

>👉 **Why it matters:** 
> 대규모 파라미터 LLM을 “외부 지식과 결합”해 쓰는 RAG 트렌드가 폭발하는 가운데, Firestarter는 “URL → 챗봇” 워크플로를 1분으로 단축해 진입장벽을 크게 낮췄다. OpenAI-호환 API를 그대로 내주므로 기존 앱·슬랙봇·플러그인에 거의 수정 없이 붙일 수 있고, 다중 LLM·서버리스 벡터 DB·스트리밍 UI 조합은 현행 생산성 툴 스택과 잘 맞물린다. 반대로 단점도 뚜렷하다:
> 1. Firecrawl·Upstash·LLM 호출 비용 및 개인정보 이슈가 외부 서비스 종속성을 남긴다.
> 2. 동적 SPA·로그인 벽 등 복잡한 사이트는 크롤링 품질이 떨어질 수 있다.
> 3. 기본 우선순위가 GPT-4o 중심이라 토큰 비용이 급증할 수 있고, 제한적 크롤링 깊이(기본 10 페이지)는 대형 사이트 지원 시 추가 튜닝이 필요하다. 그럼에도 오픈소스·구성 단순화라는 강점 덕분에, LLM 접목 챗봇·검색 서비스 분야에서 당분간 표준 툴킷 역할을 할 가능성이 높다.

### 3. MCPJam Inspector: MCP Server의 시각적 디버깅을 위한 Web UI 기반 도구
요약 없음

>👉 **Why it matters:** 
> 내용 없음

## ⭐ 4점

### 1. OpenAI Agents SDK를 사용하여 개발한 고객 서비스(CS) Agents Demo 프로젝트 공개
OpenAI 에서 발표한 LLM Agents Demo
GPT API를 활용해서 서비스를 만들기 쉽게 해줌
파이썬, next.js 기반의 데모 프로그렘을 배포중
(실제 상담사 시나리오에 기반한 다중 에이전트 오케스트레이션,
실시간 채팅, guardrail(가드레일, 주제 이탈 및 보안 탐지) 기능 포함)

>👉 **Why it matters:** 
> 서비스에 AI를 접목시키고 싶을 때 사용할만한 참고 자료

## ⭐ 3점

### 1. Can We Improve Llama 3’s Reasoning Through Post-Training Alone? ASTRO Shows +16% to +20% Benchmark Gains
파인튜닝 프레임워크 Astro에 대한 내용
논리적으로 복잡한 문제를 만났을때
MCTS(몬테카를로 트리 탐색)과 같은 과정을 통해
추론경로를 탐색하고,
CoT 로 추론 과정을 판단(추론 성공? 실패? 실패했다면 그 원인?)
해당 과정을 기록하여 새로운 학습 데이터로 사용.
복잡한 문제에 대해서
간단한 양식이 아닌 논리적이고 체계적인 양식을 사용하게됨
MATH 500, AMC 2023, AIME 2024등의 밴치마크에서 16~20%의 점수가 상승함

>👉 **Why it matters:** 
> 논리의 상승을 위해 아키텍쳐 변경이 아닌 파인튜닝 데이터 자동생성을 하고, 유의미한 성과를 거둠

### 2. LMCache: LLM 서빙 효율성을 높여주는 캐시 시스템
LMCache는 재사용 가능한 텍스트의 KV 캐시를 GPU, CPU, 디스크에 저장하고 이를 다양한 LLM 서빙 인스턴스에서 효율적으로 공유할 수 있도록 합니다.

>👉 **Why it matters:** 
> 내용 없음

### 3. Cairn: GitHub 저장소와 연동하는 오픈소스 S/W 엔지니어링 자동화 에이전트(End-to-End SWE Agent)
요약 없음

>👉 **Why it matters:** 
> 내용 없음

### 4. SmolVLA: 커뮤니티 데이터로 학습한 소규모(450M) 오픈소스 시각-언어-행동(Vision-Language-Action) 로봇 모델 (feat. Hugging Face)
허깅페이스 LeRobot 팀은 2025년 6월 3일 SmolVLA-450M을 공개했다. 이 모델은 4.5억 파라미터로 구성된 비전-언어-액션(VLA) 모델로, 공개 라이선스(아파치 2.0)와 가벼운 규모 덕분에 맥북·소비자-GPU에서도 실시간 제어가 가능하다. 사전 학습과 추론 전 과정을 오픈-소스로 제공해 로봇 연구·교육의 진입 장벽을 낮췄다.
SmolVLA는 SmolVLM-2 백본(시그LIP 비전 인코더 + SmolLM-2 언어 디코더) 위에 약 1억 파라미터의 Flow-Matching 트랜스포머 액션 전문가를 얹었다. 시각 토큰을 64개로 줄이고 VLM 상위 절반 레이어를 생략해 지연을 절반으로 줄였으며, 비동기 추론 스택으로 로봇 실행과 예측을 병렬화해 30 % 더 빠른 작업 완료와 2배 처리량을 달성했다.
모델은 **487개 커뮤니티 데이터셋(약 1,000만 프레임)**으로 사전 학습됐으며, 이 단계만으로 SO100 실제 작업 성공률이 51.7 %→78.3 %로 26.6 %p 상승했다. 그 결과, 시뮬레이션(LIBERO·Meta-World)과 실제 로봇(SO100·SO101) 모두에서 훨씬 큰 ACT 모델을 능가했고, 새로운 로봇 형태로의 일반화 테스트에서도 높은 성공률을 기록했다.

>👉 **Why it matters:** 
> 대형 VLA 모델이 연구실 전용 GPU와 사유 데이터에 묶여 있던 흐름을 “소형·오픈·커뮤니티 데이터” 축으로 전환했다는 점. SmolVLA는 1 GPU·공개 데이터만으로 ACT-급 성능을 입증해 “로봇 파운데이션 모델=초대형”이라는 통념을 깨뜨렸고, 비동기 추론·레이어 스킵 등 LLM 효율화 아이디어를 로봇 제어로 확장. 이는 저비용·실시간 로봇 에이전트의 대중화를 앞당기며, 오픈 데이터 기여가 곧 모델 성능으로 이어지는 생태계 선순환을 촉발할 가능성이 커보임.

### 5. C.O.R.E: 사용자의 지식과 상호작용할 수 있는, LLM을 위한 공유 가능한 메모리 시스템
C.O.R.E는 사용자가 완전한 소유권을 갖는 공유 가능한 메모리 시스템. 사용자의 지식과 상호작용을 시계열 기반의 지식 그래프로 구성. Cursor, Claude 등의 툴과 연동이 가능하며, 특히 SOL이라는 개인 AI 어시스턴트와 결합하여 사용자의 선호, 사실 정보, 맥락을 기반으로 더 정확하고 맞춤형 응답을 제공할 수 있도록 지원합니다.

>👉 **Why it matters:** 
> 외부에 종속되지 않으며, 사용자의 사적인 공간에서 맥락 기반의 데이터를 저장하고 활용할 수 있게 합니다. 사용자의 맥락 및 취향을 학습할 수 있도록 지원.  고급 개인 비서 및 협업 툴에서 특히 유용합니다.

## ⭐ 2점

### 1. Eion: AI 에이전트용 공동 메모리 및 지식 그래프 저장소
요약 없음

>👉 **Why it matters:** 
> 내용 없음

### 2. Context: macOS에서 MCP 서버 디버깅을 위한 네이티브 클라이
MCP의 간단한 디버깅, 모니터링을 GUI에서 다루게 해주는 “Context” 소개
2025_03 월 버전까지 호환
지원 기능:
stdio / HTTP+SSE / Streamable HTTP,
OAuth 인증 및 메타데이터 탐색,
툴 / 리소스 / 프롬프트 / 로그,
미지원:
Roots / Sampling / Completion 등 고급 기능

>👉 **Why it matters:** 
> Mcp에 대한 접근성을 높혀줌

### 3. Wrinkl: AI가 프로젝트의 맥락을 파악하고, 코드 및 문서를 일관성있게 작성하도록 돕는 AI 맥락 관리 시스템
요약 없음

>👉 **Why it matters:** 
> 내용 없음

## ⭐ 1점

### 1. Supabase MCP can leak your entire SQL database
요약 없음

>👉 **Why it matters:** 
> 내용 없음
<details>
<summary>🔽 지난 주간 소식 펼치기</summary>
<details>
<summary>[2025/07/07 ~ 2025/07/13] 주간 소식</summary>

## 주간 소식
## ⭐ 5점

### 1. Claude Code가 이제 훅(hooks)을 지원함
• Claude Code에 사용자 정의 훅 기능이 도입. LLM의 선택에 의존하지 않고, 앱의 행동을 더욱 정확하고 반복적으로 제어할 수 있음
• 알림 커스터마이징, 코드 자동 포맷팅, 명령 로그 추적과 같은 다양한 자동화가 가능
• 명령어 실행 전/후, 알림 발생, 응답 완료 시점 등에서 동작하며, 설정 파일을 통해 프로젝트·사용자·엔터프라이즈 레벨로 관리할 수 있음
• 설정 파일 구조와 매처(matcher) 방식을 통해, 특정 도구 호출 시점에 특정 훅만 실행할 수 있음
• 입력은 JSON 포맷으로 전달되고, 출력은 exit code 또는 JSON으로 결과·피드백을 제어함
• 훅은 셸 명령어를 사용자의 전체 권한으로 자동 실행하므로, 보안 및 안전에 대한 주의 필요함

>👉 **Why it matters:** 
> 내용 없음

### 2. Firestarter - 웹사이트 콘텐츠를 바탕으로 AI Chatbot을 만들어주는 오픈소스 프로젝트
Firestarter는 URL만 입력하면 웹사이트를 자동으로 크롤링·벡터화해 곧바로 RAG 기반 AI 챗봇을 배포해 주는 MIT 오픈소스 프로젝트다. Firecrawl로 콘텐츠를 Markdown 형태로 긁어오고, Upstash Search에 임베딩·인덱싱한 뒤 Next.js 15·Vercel AI SDK를 이용해 스트리밍 인터페이스와 OpenAI 호환 REST 엔드포인트를 제공한다.
사용자는 복잡한 인프라 없이도 /create API로 챗봇을 만들고 /query로 질의를 보낼 수 있으며, 생성된 챗봇은 OpenAI·Anthropic·Groq 순 우선순위에 따라 LLM을 호출한다. 크롤링 깊이·LLM 우선순위·챗봇 생성 활성화 여부 같은 파라미터는 firestarter.config.ts에서 간단히 조정할 수 있어 자가 호스팅이나 기능 확장이 용이하다.
DocsGPT / LangChain 같은 기존 RAG 툴과 달리 Firestarter는 **“즉시 사용 가능”**과 OpenAI-호환 API를 강점으로 내세운다. 개발자 문서, FAQ, 고객 지원 사이트를 빠르게 “대화형 데이터 소스”로 전환할 수 있어 커뮤니티·기업 채택이 빠르게 늘고 있다.

>👉 **Why it matters:** 
> 대규모 파라미터 LLM을 “외부 지식과 결합”해 쓰는 RAG 트렌드가 폭발하는 가운데, Firestarter는 “URL → 챗봇” 워크플로를 1분으로 단축해 진입장벽을 크게 낮췄다. OpenAI-호환 API를 그대로 내주므로 기존 앱·슬랙봇·플러그인에 거의 수정 없이 붙일 수 있고, 다중 LLM·서버리스 벡터 DB·스트리밍 UI 조합은 현행 생산성 툴 스택과 잘 맞물린다. 반대로 단점도 뚜렷하다:
> 1. Firecrawl·Upstash·LLM 호출 비용 및 개인정보 이슈가 외부 서비스 종속성을 남긴다.
> 2. 동적 SPA·로그인 벽 등 복잡한 사이트는 크롤링 품질이 떨어질 수 있다.
> 3. 기본 우선순위가 GPT-4o 중심이라 토큰 비용이 급증할 수 있고, 제한적 크롤링 깊이(기본 10 페이지)는 대형 사이트 지원 시 추가 튜닝이 필요하다. 그럼에도 오픈소스·구성 단순화라는 강점 덕분에, LLM 접목 챗봇·검색 서비스 분야에서 당분간 표준 툴킷 역할을 할 가능성이 높다.

### 3. MCPJam Inspector: MCP Server의 시각적 디버깅을 위한 Web UI 기반 도구
요약 없음

>👉 **Why it matters:** 
> 내용 없음

## ⭐ 4점

### 1. OpenAI Agents SDK를 사용하여 개발한 고객 서비스(CS) Agents Demo 프로젝트 공개
OpenAI 에서 발표한 LLM Agents Demo
GPT API를 활용해서 서비스를 만들기 쉽게 해줌
파이썬, next.js 기반의 데모 프로그렘을 배포중
(실제 상담사 시나리오에 기반한 다중 에이전트 오케스트레이션,
실시간 채팅, guardrail(가드레일, 주제 이탈 및 보안 탐지) 기능 포함)

>👉 **Why it matters:** 
> 서비스에 AI를 접목시키고 싶을 때 사용할만한 참고 자료

## ⭐ 3점

### 1. Can We Improve Llama 3’s Reasoning Through Post-Training Alone? ASTRO Shows +16% to +20% Benchmark Gains
파인튜닝 프레임워크 Astro에 대한 내용
논리적으로 복잡한 문제를 만났을때
MCTS(몬테카를로 트리 탐색)과 같은 과정을 통해
추론경로를 탐색하고,
CoT 로 추론 과정을 판단(추론 성공? 실패? 실패했다면 그 원인?)
해당 과정을 기록하여 새로운 학습 데이터로 사용.
복잡한 문제에 대해서
간단한 양식이 아닌 논리적이고 체계적인 양식을 사용하게됨
MATH 500, AMC 2023, AIME 2024등의 밴치마크에서 16~20%의 점수가 상승함

>👉 **Why it matters:** 
> 논리의 상승을 위해 아키텍쳐 변경이 아닌 파인튜닝 데이터 자동생성을 하고, 유의미한 성과를 거둠

### 2. LMCache: LLM 서빙 효율성을 높여주는 캐시 시스템
LMCache는 재사용 가능한 텍스트의 KV 캐시를 GPU, CPU, 디스크에 저장하고 이를 다양한 LLM 서빙 인스턴스에서 효율적으로 공유할 수 있도록 합니다.

>👉 **Why it matters:** 
> 내용 없음

### 3. Cairn: GitHub 저장소와 연동하는 오픈소스 S/W 엔지니어링 자동화 에이전트(End-to-End SWE Agent)
요약 없음

>👉 **Why it matters:** 
> 내용 없음

### 4. SmolVLA: 커뮤니티 데이터로 학습한 소규모(450M) 오픈소스 시각-언어-행동(Vision-Language-Action) 로봇 모델 (feat. Hugging Face)
허깅페이스 LeRobot 팀은 2025년 6월 3일 SmolVLA-450M을 공개했다. 이 모델은 4.5억 파라미터로 구성된 비전-언어-액션(VLA) 모델로, 공개 라이선스(아파치 2.0)와 가벼운 규모 덕분에 맥북·소비자-GPU에서도 실시간 제어가 가능하다. 사전 학습과 추론 전 과정을 오픈-소스로 제공해 로봇 연구·교육의 진입 장벽을 낮췄다.
SmolVLA는 SmolVLM-2 백본(시그LIP 비전 인코더 + SmolLM-2 언어 디코더) 위에 약 1억 파라미터의 Flow-Matching 트랜스포머 액션 전문가를 얹었다. 시각 토큰을 64개로 줄이고 VLM 상위 절반 레이어를 생략해 지연을 절반으로 줄였으며, 비동기 추론 스택으로 로봇 실행과 예측을 병렬화해 30 % 더 빠른 작업 완료와 2배 처리량을 달성했다.
모델은 **487개 커뮤니티 데이터셋(약 1,000만 프레임)**으로 사전 학습됐으며, 이 단계만으로 SO100 실제 작업 성공률이 51.7 %→78.3 %로 26.6 %p 상승했다. 그 결과, 시뮬레이션(LIBERO·Meta-World)과 실제 로봇(SO100·SO101) 모두에서 훨씬 큰 ACT 모델을 능가했고, 새로운 로봇 형태로의 일반화 테스트에서도 높은 성공률을 기록했다.

>👉 **Why it matters:** 
> 대형 VLA 모델이 연구실 전용 GPU와 사유 데이터에 묶여 있던 흐름을 “소형·오픈·커뮤니티 데이터” 축으로 전환했다는 점. SmolVLA는 1 GPU·공개 데이터만으로 ACT-급 성능을 입증해 “로봇 파운데이션 모델=초대형”이라는 통념을 깨뜨렸고, 비동기 추론·레이어 스킵 등 LLM 효율화 아이디어를 로봇 제어로 확장. 이는 저비용·실시간 로봇 에이전트의 대중화를 앞당기며, 오픈 데이터 기여가 곧 모델 성능으로 이어지는 생태계 선순환을 촉발할 가능성이 커보임.

### 5. C.O.R.E: 사용자의 지식과 상호작용할 수 있는, LLM을 위한 공유 가능한 메모리 시스템
C.O.R.E는 사용자가 완전한 소유권을 갖는 공유 가능한 메모리 시스템. 사용자의 지식과 상호작용을 시계열 기반의 지식 그래프로 구성. Cursor, Claude 등의 툴과 연동이 가능하며, 특히 SOL이라는 개인 AI 어시스턴트와 결합하여 사용자의 선호, 사실 정보, 맥락을 기반으로 더 정확하고 맞춤형 응답을 제공할 수 있도록 지원합니다.

>👉 **Why it matters:** 
> 외부에 종속되지 않으며, 사용자의 사적인 공간에서 맥락 기반의 데이터를 저장하고 활용할 수 있게 합니다. 사용자의 맥락 및 취향을 학습할 수 있도록 지원.  고급 개인 비서 및 협업 툴에서 특히 유용합니다.

## ⭐ 2점

### 1. Eion: AI 에이전트용 공동 메모리 및 지식 그래프 저장소
요약 없음

>👉 **Why it matters:** 
> 내용 없음

### 2. Context: macOS에서 MCP 서버 디버깅을 위한 네이티브 클라이
MCP의 간단한 디버깅, 모니터링을 GUI에서 다루게 해주는 “Context” 소개
2025_03 월 버전까지 호환
지원 기능:
stdio / HTTP+SSE / Streamable HTTP,
OAuth 인증 및 메타데이터 탐색,
툴 / 리소스 / 프롬프트 / 로그,
미지원:
Roots / Sampling / Completion 등 고급 기능

>👉 **Why it matters:** 
> Mcp에 대한 접근성을 높혀줌

### 3. Wrinkl: AI가 프로젝트의 맥락을 파악하고, 코드 및 문서를 일관성있게 작성하도록 돕는 AI 맥락 관리 시스템
요약 없음

>👉 **Why it matters:** 
> 내용 없음

## ⭐ 1점

### 1. Supabase MCP can leak your entire SQL database
요약 없음

>👉 **Why it matters:** 
> 내용 없음

</details>
</details>